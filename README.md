
# ğŸŒ¦ï¸The Climate Compass: Weather Forecasting

## ğŸ“– Project Overview
This project demonstrates a **modern BI architecture on Microsoft Fabric** for ingesting, transforming, and visualizing **real-time weather data**.  

I implement a **hybrid refresh approach**:
- **Automatic ingestion every 60 minutes** â†’ ensures continuous **historical dataset**.  
- **On-demand refresh via a Power BI button** â†’ users can fetch the **latest weather snapshot instantly**.  

The solution follows a **Medallion Architecture Bronze â†’ Silver â†’ Gold** design pattern, separating raw storage, cleaned append logs, and deduplicated reporting data.

---
## ğŸ› ï¸ Tools & Technologies

This project integrates Data Engineering + Business Intelligence capabilities using modern tools:

- **Microsoft Fabric**
   - Unified Data & Analytics SaaS platform.
   - Provides Lakehouse, Dataflow Gen2, Pipelines, and DirectLake for seamless data movement and reporting.
   - Enables Medallion Architecture (Bronze â†’ Silver â†’ Gold).

- **Python (with Pandas & Requests)**
   - Crawler Notebook written in Python scrapes live weather APIs/websites.
   - Pandas used for data wrangling (cleaning, transforming CSV files).
   - Handles timestamps, city-level records, and saves CSVs to OneLake.

- **SQL (Fabric SQL Endpoint)**
   - Used for validation and monitoring queries.
   - Ensures deduplication, data coverage by city, and latest timestamps.
   - Helps in data quality checks on Gold tables.

- **Power BI**
   - Visualizes historical trends and latest weather snapshots.
   - Connects directly to Gold table via DirectLake â†’ no dataset refresh needed.
   - Includes â€œGet Latest Dataâ€ button (Power Automate trigger).

- **Power Query / Dataflow Gen2**
   - Transforms raw Bronze files into Silver (append log) and Gold (deduplicated) Delta tables.
   - Provides lineage columns (source_file, crawl_time, ingestion_time).
   - Supports incremental and automated transformations.

- **Data Pipelines (Fabric Pipeline)**
   - Orchestrates workflow: Crawler Notebook â†’ Wait â†’ Dataflow refresh.
   - Scheduled hourly runs + on-demand triggers (via Power Automate).
   - Handles retries, timeouts, and monitoring.

- **Business Intelligence & Data Engineering Integration**
   - BI layer (Power BI) provides insights, trends, and live snapshots.
   - DE layer (Fabric Lakehouse, Pipelines, Dataflow) ensures scalable ingestion, transformation, and storage.

Together, they deliver an end-to-end robust BI + DE solution.

## ğŸ—ï¸ Architecture

### Components
1. **Crawler Notebook (Fabric Notebook)**  
   - Scrapes weather data for multiple cities.  
   - Saves raw CSV files into:  
     ```
     Files/forecast_data_broze/weather_forecast {yyyy-MM-dd HH:mm:ss}.csv
     ```
   - Example: `weather_forecast 2025-08-27 10:15:30.csv`.

2. **Lakehouse (WeatherLakehouse)**  
   - Stores raw files in `Files/weather_data_broze`.  
   - Contains **Silver** and **Gold** Delta tables.

3. **Dataflow Gen2 (Weather Dataflow)**  
   - **Silver Table (`weather_data_silver`)**  
     - Appends all crawled datasets with lineage columns:  
       - `source_file`, `file_crawl_time`, `ingestion_time`.  
   - **Gold Table (`weather_data_gold`)**  
     - Deduplicated by `(city, date_time)`.  
     - Columns renamed for clarity (e.g. `temperature_c`, `humidity_pct`).  
     - Used directly by Power BI.

4. **Pipeline (WeatherHourlyPipeline)**  
   - Orchestrates:
     1. Run **Notebook (Data_Crawler)**.  
     2. **Wait 20â€“30 seconds** (file commit buffer).  
     3. Refresh **Dataflow Gen2**.  
   - Runs **every 60 minutes (schedule)**.  
   - Also exposed to Power BI button via **Power Automate**.

5. **Power BI Dashboard**  
   - Connects directly to `weather_data_gold` via **DirectLake**.  
   - Pages:
     - **Overview**: historical temperature/humidity trends by city.  
     - **Snapshot**: latest temperature/humidity by city.  
   - Includes **â€œGet Latest Dataâ€ button** (Power Automate â†’ Run Pipeline).

---

## ğŸ”„ Data Flow

```
Notebook (Crawler)
    â†“
Raw CSV files (Files/weather_data_broze in WeatherLakehouse)
    â†“
Dataflow Gen2
    â”œâ”€ Silver Table: weather_data_silver (append log)
    â””â”€ Gold Table: weather_data_gold (deduped, reporting)
    â†“
Pipeline (orchestrates Notebook â†’ Wait â†’ Dataflow refresh)
    â†“
Power BI (DirectLake â†’ Gold table)
    â†“
Users (historical insights + live button refresh)
```

---

## âš™ï¸ Pipeline Configuration

**Activities:**
1. **Data_Crawler (Notebook)**  
   - Timeout: `0:10:00`  
   - Retry: `2`  
   - Retry interval: `120 sec`

2. **Wait**  
   - Duration: `20â€“30 sec` (ensures files are committed in OneLake)

3. **Refresh_Weather_Dataflow (Dataflow Gen2)**  
   - Timeout: `0:20:00`  
   - Retry: `2`  
   - Retry interval: `120 sec`

**Schedule:** Every **60 minutes**, no concurrent runs.  

**Trigger:** Also exposed via Power Automate for **on-demand refresh**.

---

## ğŸ“Š Power BI Setup

### Connection
- Dataset: **weather_data_gold** (DirectLake from Lakehouse).  
- DirectLake ensures:
  - No dataset refresh required.  
  - Reports always show the current Gold table content.

### Pages
- **Overview**:  
  - Line chart â†’ `date_time` vs `temperature_c` by `city`.  
  - Card visuals â†’ latest metrics.  
- **Snapshot**:  
  - Table of latest weather by city (`Latest Timestamp`).  
  - Button â†’ â€œGet Latest Dataâ€ (Power Automate).

---

## ğŸš€ Hybrid Refresh Approach

- **Hourly schedule**: Keeps history complete and up-to-date.  
- **On-demand button**: Fetches live weather instantly.  
- Ensures both:  
  - Historical dataset for analysis.  
  - Live, ad-hoc updates when needed.

---

## âœ… Validation & Monitoring

### SQL Checks (Lakehouse SQL Endpoint)
```sql
-- Latest record timestamp
SELECT MAX(date_time) AS latest_ts FROM weather_data_gold;

-- Coverage by city
SELECT city, COUNT(*) AS rows_, MIN(date_time) AS first_dt, MAX(date_time) AS last_dt
FROM weather_data_gold
GROUP BY city;

-- Check duplicates
SELECT city, date_time, COUNT(*) c
FROM weather_data_gold
GROUP BY city, date_time
HAVING COUNT(*) > 1;
```

### Monitoring
- **Pipeline run history** â†’ check success/failures.  
- **Dataflow refresh history** â†’ row counts.  
- **Power BI card** â†’ shows `Latest Timestamp` for confidence.

---

## ğŸ› ï¸ Best Practices

- **Keep raw files untouched** â†’ always stored in `Files/weather_data_broze`.  
- **Use Silver for append logs** â†’ preserves lineage.  
- **Use Gold for reporting** â†’ deduped, clean, optimized.  
- **DirectLake for Power BI** â†’ no dataset refresh needed.  
- **Retry & Timeout settings** â†’ handle transient failures gracefully.  
- **Single source of refresh (Pipeline)** â†’ avoid dedup errors.  

---

## ğŸ“Œ Next Steps / Extensions

- Add **incremental refresh policy** in Dataflow Gen2 (e.g., keep 12 months, refresh last 3 days).  
- Extend crawler to fetch additional metrics (e.g., AQI, sunrise/sunset).  
- Add **alerts** (Teams/Email) when pipeline fails.  
- Integrate **real-time hub** â†’ auto-run pipeline when new CSV lands.  
- Publish Power BI app with role-based access (city/region-level RLS).  

---

## ğŸ—‚ï¸ Repository Structure
```
/WeatherProject
â”‚
â”œâ”€ /notebooks
â”‚   â””â”€ crawler_notebook.ipynb
â”‚
â”œâ”€ /dataflows
â”‚   â””â”€ weather_dataflow.json
â”‚
â”œâ”€ /pipelines
â”‚   â””â”€ WeatherHourlyPipeline.json
â”‚
â”œâ”€ /powerbi
â”‚   â””â”€ WeatherReport.pbix
â”‚
â””â”€ README.md   â† (this file)
```

---

ğŸ’¡ With this setup, you get **a robust end-to-end BI solution**:  
âœ” Continuous history  
âœ” Live updates on demand  
âœ” Scalable Lakehouse storage  
âœ” Seamless DirectLake reporting  
